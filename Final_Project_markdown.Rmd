---
title: "Final_Project_markdown"
author: "Pauline Palma"
date: "03/04/2020"
output: github_document
---

```{r echo = FALSE, message = FALSE}
library(dplyr)
library(lme4)
library(lmerTest)
library(lattice)
library(tidyverse)
library(wesanderson)
library(effects)
library(papaja)
options(scipen = 999)
```

# 1. Description of the learning objectives

Welcome to this demo!

Today, you will learn about cross-classified multilevel models, a type of model that is commonly found in various domains of experimental psychology, such as social psychology or psycholinguistics. 

A cross-classified multilevel model allows one to estimate the unique influence of two
distinct Level 2 clusters. For instance, cross-classified MLM have been used in social psychology to assess variance in social impression formation (for a review, see Xie et al., 2018). In a social interaction, the impressions formed depend both on perceiver and on target characteristics, such as race and gender. Cross-classified MLM are especially useful there, as they allow for the partitioning of variance in impressions between perceivers and targets. 

Cross-classified MLM are also common in psycholinguistics, as they allow researchers to estimate the unique influence of individual participants and of infividual items, such as words or sentences. This is especially useful, as psycholinguists are usually interested in both participants' characteristics (e.g., their age, native language, etc.) and items' characteristics (e.g., word length, frequency, etc.). 

In the present demo, we try to model total reading time (an eye-tracking measure) as a function of participant's native language and item's sentence context. Cross-classified MLM allow us to account for participant variability (e.g., some participants might be slower than others) and for item variability (e.g., some words might be less familiar to the participants).

# 2. Research questions
Human languages are inherently ambiguous. An important aspect of language processing is to make sense of this ambiguity. For example, when presented with the homonym sage, one must choose between the meaning ‘herb’ and the meaning ‘wise man’. Research on monolingual readers has shown that these multiple meanings are activated in parallel, which generates competition for meaning selection (e.g., Duffy, Kambe & Rayner, 2001; Rayner, Pacht & Duffy, 1994). Contextual sentence has been shown to cue meaning selection, hence reducing reading times on the ambiguous word (Rayner et al., 1994).

Importantly, reading speed is also impacted by participants' characteristics. For instance, non-native readers usually exhibit slower reading time compared to native readers, due to lower reading proficiency (e.g., ). Older adults are also slower compared to younger readers, due to slower processing speed (e.g., ). There is also some evidence that contextual sentence specifically impacts reading times for ambiguous words in non-native speakers (Palma, Whitford, & Titone, 2019; see also Arêas da Luz Fontes & Schwartz, 2010, 2015; Schwartz, Yeh, & Shaw, 2008). However, it is unclear if these effects extend to older bilingual speakers, as participants in these studies were young adults (18-35 years old). Because the effects of bilingualism have been shown to change across the lifespan (Whitford & Titone, 2019), it would be interesting to extend these results to older samples. 

The goal with this project is to analyze how bilingual older adults (60+ years old) process ambiguous English words during natural reading. Older bilingual adults read English sentences containing homonyms on a screen, while their eye movements were recorded. Participants were either French-English (reading in their second language) or English-French (reading in their first language) older bilinguals. The homonyms were embedded in sentences that either contained a modifier (e.g., adjective) related to one of the two meanings (e.g., Mary liked the fresh/elder sage), or did not contain such a modifier (e.g. mary liked the sage).

We constructed three models (including a null model) to answer four specific research questions: 

a) Do bilingual older adults process ambiguous words differently when they are embedded in sentences that clarify their meaning or not? -> Model 2
b) Does the effect of context vary across participants? -> Model 3
c) Does native language modulate the processing of ambiguous words? -> Model 3
d) Does context impact impact the processing of ambiguous words regardless of native language? -> Model 3

# 3. Description of the variables
```{r}
PSYC746_final <- read.csv("PSYC746_final.csv")
```

```{r}
summary(PSYC746_final)
length(unique(PSYC746_final$Subject))
length(unique(PSYC746_final$ITEM))
```
There are 32 participants and 64 items in the dataset. There are 1988 lines, representing the 1988 trials completed by all participants.

* X: trial number 
* Subject: Subject ID number 
* ITEM: Item ID number 
* log_TRT_target: log of the total reading time on the target ambiguous word. Total reading time is the sum of all the fixations made on the word in milliseconds, measured with an eye-tracking system. This measure is thought to reflect ambiguity resolution and meaning integration processes. It was logged to correct for skew. This is the dependent variable in the models.
* Group_dev: language group, that is, whether the first language acquired by the participant was English (English as a first language) or French (French as a first language). This was assessed via a language history questionnaire. This variable was effects-coded (-.5: English as a first language, .5: French as a first language). This is a characteristic of participants (level-2) and a predictor.
* Context_dev: sentence context, that is, whether the sentence contained a modifier (usually, an adjective) related to one of the homonym’s meanings or not. This variable was effects-coded (-.5: no modifier, .5: modifier). This is a characteristic of items (level-2) and a predictor.

# 4. Description/visual of the data structure

We first verify the functional form of the variables of interest by plotting them. 

* Dependent variable
```{r}
hist(PSYC746_final$log_TRT_target)
```
The DV is not perfectly normally distributed (skew to the right).


* Relationship predictors & dependent variable
```{r}
PSYC746_final %>% ggplot(mapping = aes(x = Context_dev, y = log_TRT_target)) +
  geom_bar(stat="identity", position="dodge", aes(fill=Context_dev)) +
  facet_wrap(~Group_dev) +
  labs(x="Context", y = "TRT of the target ambiguous word\n(log ms)", fill="") +
  coord_cartesian(ylim=c(6,9)) +
  theme_apa()+
  theme_bw(base_size = 20) +
  theme(legend.position = "none", axis.text.x = element_text(size = 16), 
        axis.text.y = element_text(size = 16), axis.title.x= element_text(size = 16),
        axis.title.y = element_text(size = 12), strip.text.x = element_text(size = 16),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.border = element_rect(colour = "black", fill=NA, size=1))
  
```

* Correlation predictors & dependent variable
```{r}
PSYC746_final %>% 
  select(log_TRT_target, Group_dev, Context_dev) %>%
  cor()
```
.20 correlation between Group and log TRT
-.14 correlation between Context and log TRT  

* Individual scatterplots

```{r}
PSYC746_final %>% 
  ggplot() +
  geom_point(mapping = aes(x = Context_dev, y = log_TRT_target)) +
  facet_wrap(~Subject) +
  coord_cartesian(ylim=c(4,10))
```
Most participants follow the expected pattern, in that they exhibit faster log TRT when Context = .5 (i.e., when a modifier is present). Some participants (e.g., 19) do not seem to follow this pattern.  

```{r}
PSYC746_final %>% 
  ggplot() +
  geom_point(mapping = aes(x = Context_dev, y = log_TRT_target)) +
  facet_wrap(~ITEM) +
  coord_cartesian(ylim=c(4,10))
```
Most items follow the expected pattern, in that they are associated with lower log TRT when a modifier related to one of their meanings is present in the sentence. Not all items follow this pattern (item 2, item 50 for example).

```{r}
PSYC746_final %>% 
  ggplot() +
  geom_point(mapping = aes(x = Group_dev, y = log_TRT_target)) +
  facet_wrap(~ITEM)+
  coord_cartesian(ylim=c(4,10))
```
The pattern is unclear--it looks like some items, but not all, were read faster by bilingual participants with English as their native language.

* Regression line

```{r}
PSYC746_final %>% 
  group_by(Subject) %>% 
ggplot(mapping = aes(x = Context_dev, y = log_TRT_target, colour = factor(Subject))) +
  geom_point(show.legend = FALSE) +
  geom_smooth(method = lm, se = FALSE, show.legend = FALSE)
```
At the subject level, a modifier in the sentence context (Context_dev = .5) is associated with decreased log TRT.

Context by item
```{r}
PSYC746_final %>% 
  group_by(ITEM) %>% 
ggplot(mapping = aes(x = Context_dev, y = log_TRT_target, colour = factor(ITEM))) +
  geom_point(show.legend = FALSE) +
  geom_smooth(method = lm, se = FALSE, show.legend = FALSE)
```
At the item level, it is less clear that a modifier in the sentence context (Context_dev = .5) is associated with decreased log TRT (some items are associated with higher log TRT in such sentence contexts.

```{r}
PSYC746_final %>% 
  group_by(ITEM) %>% 
ggplot(mapping = aes(x = Group_dev, y = log_TRT_target, colour = factor(ITEM))) +
  geom_point(show.legend = FALSE) +
  geom_smooth(method = lm, se = FALSE, show.legend = FALSE)
```
Items are associated with increased log TRT when they are read by participants whose native language is French (Group_dev: .5).

# 5. Models and interpretation

## a. Model 1 (null)
### Equation
The equation for a cross-classified model is different from the usual MLM equation. Below, we spelled out all the terms.

L1: TRTi(j1k1) = 𝜋0(j1k1) + ei(j1k1)
L2: 𝜋0(j1k1) = 𝜃000 + b0j10 + c00k1 

We estimate 4 parameters (1 fixed effect, 3 random effects) with this null model.
Level 1 is the trial level
TRTi(j1k1) is the TRT of the trial for item j1 and participant k1
𝜋0(j1k1) is the expected value for TRT for this item and this participant
ei(j1k1) is the error term, which accounts for error for variability with respect to participant and item level

Level2 is the cluster level: there are two distinct clusters, such that trials are grouped both under participants and under items.

𝜋0(j1k1) is the expected value for TRT, that can vary across participants and items, which allows the total variance of the model to be partitioned into that attributable to participants and items
𝜃000 is the grand mean for TRT, averaged across all items and participants
b0j10 is the residual of the item j1 averaged across all participants (i.e., the difference between the grand mean and the TRT of target j1), which has variance tb00 c00k1 is the residual of participant k1 averaged across all items (i.e., the difference between the grand mean and the TRT of participant k1), which has variance tc00

*Usually, cross-classified MLM also include another term, d0(j1k1), which is the interaction between item and participant variance in the model. However, to be able to estimate the random effect associated with this interaction requires repeated reading of a single item by each participant, in order to separate the variance associated with this interaction from the residual variance. In the present study, each participant read each item only once, so we are unable to estimate this component and it is fixed to zero (not estimated). We have therefore only have three variance terms in the model: variance across items, variance across participants, and variance of the L1 error term ei(j1k1). 

### Model
```{r}
Null = lmer(log_TRT_target ~ 1+(1|Subject) + (1|ITEM), data=PSYC746_final, REML = T)
summary(Null)
```


### Interpretation
```{r}
exp(6.22055)
```

Interpretation:
The fixed effects table indicates that the expected average value for TRT, across all participants and items, is 6.22055 log ms, so 502.9798 ms. 
Notice that, in the random effects table, three random effects can be found: the random intercept for ITEM, the random intercept for Subject, and the residuals. 

In what follows, we explore how important the clusters are in the MLM.  

### Exploration of the random effects

#### Participants
We first calculate the Intra-Class Correlation, in order to assess the proportion of variance that is explained by the clusters. The ICC is calculated slightly differently in a cross-classified MLM compared to a regular MLM, as it takes into account the presence of two clusters in the model. The ICC in a cross-classified data structure is calculated by dividing the random effect (by participant or by items) by the total variance (by participant + by item + residuals) (see Carson & Beeson, 2013; Locker et al., 2007).
```{r ICC participant}
(0.20575)/(0.20575+0.02596+0.27660)
```
40.48% of the total variance is explained by between-subject variability. 

We then calculate the Design effect to assess whether a cluster effect is present for participants. 

We first calculate the mean number of observation per participant
```{r}
x = as.data.frame(table(PSYC746_final$Subject))
mean(x$Freq)
```
Mean number of observations per participant = 62.125

```{r}
DEFF = 1+(62.125-1)*0.4047727
```
DEFF participants = 25.742
Because this number is much higher than 1, it suggests an important cluster effect for participants.

Effective sample size
```{r}
32/25.742
```
The effective sample size, when adjusted for participant clustering, is 1.243.

We then calculate the DEFT, in order to get a sense of how off our standard error estimate would be in a model that is not a MLM.
```{r}
DEFT = sqrt(DEFF)
```
Standard errors are 5.074 times larger than was originally estimated in a normal analysis.

#### Items
We first calculate the ICC for the item cluster.
```{r ICC item}
(0.02596)/(0.02596+0.20575+0.27660)
```
5.11% of the total variance is explained by between-item variability.
40.48% 

We then calculate the Design effect to assess whether a cluster effect is present for items. We first calculate the mean number of observations per item.
```{r}
x = as.data.frame(table(PSYC746_final$ITEM))
mean(x$Freq)
```
Mean number of observations per item = 31.063

```{r}
DEFF2 = 1+0.0510712*(31.063-1)
```
DEFF items = 2.535
Because this number is higher than 1, it suggests that there is a cluster effect for items. 

Effective sample size
```{r}
64/2.535
```
The effective sample size, when adjusted for participant clustering, is 25.247.

We then calculate the DEFT, in order to get a sense of how off our standard error estimate would be in a model that is not a MLM.

```{r}
DEFT2 = sqrt(DEFF2)
```
Standard errors are 1.592 times larger than was originally estimated in a normal analysis.

Overall, these scores warrant the use of a MLM for this dataset. We therefore continue our analysis by adding a level-2 predictor to the null model.

## Model 2
### Equation
L1: TRTi(j1k1) = 𝜋0(j1k1) + ei(j1k1)
L2: 𝜋0(j1k1) = 𝜃000 + 𝜃001Contextj+ b0j10 + c00k1

In this model, we include the effects-coded level-2 predictor "Context_dev", capturing the difference between embedding sentence contexts with or without a modifier related to one of the homonym's meanings (e.g., Mary liked the [fresh/elder] sage). Thus, we estimate 5 parameters in this model (2 fixed effects, 3 random effects). 
This model allows us to answer question a) Do bilingual older adults process ambiguous words differently when they are embedded in sentences that clarify their meaning or not? 

### Model
```{r}
Model2 = lmer(log_TRT_target ~ Context_dev +
                (1|Subject) + 
                (1|ITEM), data=PSYC746_final, REML = T)
summary(Model2)
```

```{r}
exp(6.22001)
```

### Interpretation
Remember that the predictor "Context_dev" is effects-coded (-.5 = no modifier, .5 = modifier present). 
The fixed effects table indicates that the expected average value for TRT across all participants and items, when Context_dev is at 0 (i.e., while controlling for sentence context), is 6.22001 log ms, so 502.7083 ms. 
There is a significant negative effect of Context_dev on expected TRT, suggesting that homonyms embedded in sentence contexts with a modifier related to one of their target meanings are read faster than compared to the average reading speed across both sentence contexts. In the next section, we plotted this effect. This suggests that bilingual older adults do process ambiguous words differently when they are embedded in sentences that clarify their meaning. 

### Visualization of the effect
```{r}
ef <- as.data.frame(Effect(c("Context_dev"), Model2))
ef
ef$Context = ifelse(ef$Context_dev == -0.5, "Bare", ifelse(ef$Context_dev == 0.5, "Modifier", "HDUEHEIQU"))
ef = subset(ef, Context != "HDUEHEIQU")

ggplot(ef, aes(x=Context, y=fit)) +
  geom_bar(stat="identity", position="dodge", aes(fill=Context)) +
  scale_fill_manual(values = wes_palette("GrandBudapest2"))+
  geom_errorbar(aes(ymin=fit-se, ymax=fit+se), width=.3, size = .6) +
  labs(y="TRT of the target ambiguous word\n(log ms, fitted)", x = "Prior sentence context", fill="") +
  coord_cartesian(ylim=c(5,7)) +
  theme_apa()+
  theme_bw(base_size = 20) +
  theme(legend.position = "none", axis.text.x = element_text(size = 16), 
        axis.text.y = element_text(size = 16), axis.title.x= element_text(size = 16),
        axis.title.y = element_text(size = 12), strip.text.x = element_text(size = 16),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.border = element_rect(colour = "black", fill=NA, size=1))
```

In order to reduce the length of this demo, we do not run additional checks on this model. However, we fully check model assumptions for model 3, our last model, and compare it to Model 2 and the null model in terms of fit.

## Model 3
### Equation
L1: TRTi(j1k1) = 𝜋0(j1k1) + ei(j1k1)
L2: 𝜋0(j1k1) = 𝜃000 + b0j10 + c00k1 + 𝜃001Contextj + 𝜃002Language groupk + c10k1 + 𝜃001Language groupk* 𝜃002Contextj 

We estimate 8 parameters in this model (4 fixed effects, 4 random effects). We added a random slope for Context_dev by participant, in order to assess whether the effect of context varies across participants. This model also includes a novel level 2 predictor for language group, capturing the difference between participants with English as their first vs. second language. Because we were interested in how the effect of sentence context depended on the effect of language group, we also included the interaction of these two predictors. 
Adding these parameters allows us to answer questions b), c) and d).
b) Does the effect of context vary across participants? 
c) Does native language modulate the processing of ambiguous words?
d) Does context impact the processing of ambiguous words regardless of native language?

### Model
```{r}
Model3 = lmer(log_TRT_target ~ Context_dev*Group_dev+
                (1+Context_dev||Subject) + 
                (1|ITEM), data=PSYC746_final, REML = T)
summary(Model3)
```

```{r}
exp(6.19294)
```

#### Interpretation
Inspection of the fixed effect table indicates that the expected value for log TRT when Context and Group are at 0 (i.e., for both sentence context type and both language groups), is 6.19294 log ms, so 489.2825ms.
There is a significant negative effect of context on expected TRT, suggesting that homonyms embedded in sentence contexts with a modifier related to one of their target meanings are read faster than compared to the average reading speed across both sentence contexts.
The effect of language group does not reach significance, suggesting that no  difference of French-English bilinguals, in terms of Log TRT, from the grand mean log TRT of the two language groups. This indicates that native language does note modulate the processing of ambiguous words (question c).
The interaction of language group and context also does not reach significance, suggesting that context does impacts the processing of ambiguous words regardless of native language (question d).


#### Visualization of the interaction (non-significant)
```{r}
ef <- as.data.frame(Effect(c("Context_dev", "Group_dev"), Model3))
ef
ef$Context = ifelse(ef$Context_dev == -0.5, "Bare", ifelse(ef$Context_dev == 0.5, "Modifier", "HDUEHEIQU"))
ef$Group = ifelse(ef$Group_dev == -0.5, "L1 English", ifelse(ef$Group_dev == 0.5, "L1 French", "HDUEHEIQU"))
ef = subset(ef, Context != "HDUEHEIQU" & Group != "HDUEHEIQU")

ggplot(ef, aes(x=Context, y=fit)) +
  geom_bar(stat="identity", position="dodge", aes(fill=Context)) +
  scale_fill_manual(values = wes_palette("GrandBudapest2"))+
  geom_errorbar(aes(ymin=fit-se, ymax=fit+se), width=.3, size = .6) +
  labs(y="TRT of the target ambiguous word\n(log ms, fitted)", x = "Prior sentence context", fill="") +
  facet_wrap(~Group)+
  coord_cartesian(ylim=c(4,7)) +
  theme_apa()+
  theme_bw(base_size = 20) +
  theme(legend.position = "none", axis.text.x = element_text(size = 16), 
        axis.text.y = element_text(size = 16), axis.title.x= element_text(size = 16),
        axis.title.y = element_text(size = 12), strip.text.x = element_text(size = 16),
        panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.border = element_rect(colour = "black", fill=NA, size=1))
```

#### 95% plausible values range for intercepts

##### How much do means vary across participants?
```{r}
SDx2 <- (2*(sqrt(0.190874)))
Upper_range <- 6.19294 + SDx2
lower_range <- 6.19294 - SDx2
```
The 95% plausible values range for participants means is 5.319-7.067 log ms.

##### How much do means vary across items?

```{r}
SDx2 <- (2*(sqrt(0.026274)))
Upper_range <- 6.19294 + SDx2
lower_range <- 6.19294 - SDx2
```
The 95% plausible values range for participants means is 5.869-6.517 log ms.


### Model comparison

#### Deviance tests
```{r}
anova(Model2, Model3, refit = FALSE)
```
Model 3 is not superior to Model 2 in terms of fit (X2(3) = 1.524, p > .05). The Log likelihood of Model 2 and Model 3 are similar.

```{r}
anova(Null, Model2, refit = FALSE)
```

Model 2 is superior to the null model in terms of fit (X2(1) = 61.064, p < .001).

#### Profile confidence intervals
```{r}
confint(Model3, oldNames = FALSE) 
```
The profile confidence interval for language group, context*language group, and for the random slope for context by subject all include 0. This suggests that these predictors and the random slope do not contribute significantly to the model and allows us to answer "no" to question b (Does the effect of context vary across participants?).

#### Difference in residuals
Model comparison suggests that Model 2 is superior to Model 3 in terms of fit. We now compare the residuals of Model 2 and 3.

```{r}
tau2change_p = 0.20535-0.190874
```
.014

```{r}
tau2change_i = 0.02621-0.026274
```
-.000

```{r}
sigma2change = 0.26718-0.266246
```
.001

Adding the language group predictor, the interaction between language group and context, and the random slope for context by participant does not really impact the residuals.

#### Variance reduction
We now assess by how much variance at level 1 and 2 is reduced in Model 3 compared to Model 2.

```{r}
L1_var_reduction <- sigma2change/0.26718
```
.003

```{r}
L2_var_reduction_p <- tau2change_p/0.20535
```
.070

```{r}
L2_var_reduction_i <- tau2change_i/0.02621
```
-.002

Adding the language group predictor, the interaction between language group and context, and the random slope for context by participant slightly reduces the variance explained by participant variability (-7%).

#### Conditional ICC
We now calculate the change in ICC caused by the addition of predictors and the random slope.
```{r}
conditionalICC_p = 0.190874/ (0.190874 + 0.026274+0.266246)
conditionalICC_i = 0.026274 / (0.026274 + 0.190874+ 0.266246)
```
In Model 3, 39.49% of the total variance is due to variability between subjects and 5.44% of the variance is due to variability between items. As a reminder, the ICC of the null model suggested that 40.48% of the total variance was due to variability between subjects and 5.11% of the total variance was explained by between-item variability. Therefore, Model 3 is associated with a slight decrease in the ICC by participant, and a slight increase of the ICC by item.

### Assumption checks

#### Distribution of L1 residuals
```{r}
l1_residuals <- tibble::enframe(residuals(Model3))
PSYC746_final <- PSYC746_final %>% 
  bind_cols(l1_residuals) %>% 
  select(-name) %>% 
  rename(l1resid = value)
```

```{r, echo = FALSE, warning = FALSE}
PSYC746_final %>% ggplot(mapping = aes(x = l1resid)) +
  geom_histogram()
```
L1 residuals look fairly normal.

#### Distribution of L2 residuals
```{r}
par(mfrow=c(1,3))

PSYC746_finalByParticRanef <- ranef(Model3)$Subject[['(Intercept)']]
qqnorm(PSYC746_finalByParticRanef,  main = "participant intercepts") #2 weirdos
qqline(PSYC746_finalByParticRanef)

PSYC746_finalByParticRanef_slope <- ranef(Model3)$Subject[['Context_dev']]
qqnorm(PSYC746_finalByParticRanef_slope,  main = "participant slopes") #OK
qqline(PSYC746_finalByParticRanef_slope)

PSYC746_finalByItemRanef <- ranef(Model3)$ITEM[['(Intercept)']]
qqnorm(PSYC746_finalByItemRanef,  main = "item intercepts") #1 weirdo
qqline(PSYC746_finalByItemRanef)
```

There is a potential violation of the assumption of normality of residuals. There are two participants that should be flagged as outliers, and potentially one item that should be flagged as an outlier. 

This is confirmed by the plots of the L2 residuals below

L2 residuals-participants (intercept)

```{r, echo = FALSE, warning = FALSE}
l2_residuals_p <- ranef(Model3)$Subject
l2_residuals_p %>% 
  ggplot(mapping = aes(x = `(Intercept)`)) +
  geom_histogram()
```

L2 residuals - Items (intercept)
```{r, echo = FALSE, warning  = FALSE}
l2_residuals_i <- ranef(Model3)$ITEM
l2_residuals_i %>% 
  ggplot(mapping = aes(x = `(Intercept)`)) +
  geom_histogram()
```

L2 residuals - Participants (slope)
```{r}
l2_residuals_p %>% 
  ggplot(mapping = aes(x = Context_dev)) +
  geom_histogram()
```

#### Summary statistics L2 residuals

```{r}
summary(PSYC746_finalByParticRanef)
summary(PSYC746_finalByParticRanef_slope)
summary(PSYC746_finalByItemRanef)
```
All L2 residuals have a mean of 0 but their median is not 0, which suggests some nonnormality in their distribution.

#### Flag outliers-participants (intercept)

```{r, echo = FALSE}
is_outlier <- function(x) {
  return(x < quantile(x, 0.25) - 1.5 * IQR(x) | x > quantile(x, 0.75) + 1.5 * IQR(x))
}

temp_int <- l2_residuals_p %>% 
  tibble::rownames_to_column(var="outlier") %>% 
  mutate(is_outlier = ifelse(is_outlier(`(Intercept)`), `(Intercept)`, as.numeric(NA)))
temp_int$outlier[which(is.na(temp_int$is_outlier))] <- as.numeric(NA)

ggplot(temp_int, aes(y = `(Intercept)`, x = 0)) +
  geom_boxplot()  +
  geom_text(aes(label = outlier), na.rm = TRUE, nudge_y = 0.2)
```
Participant 29 and 8 are outliers.

#### Flag outliers-items (intercept)
```{r, echo = FALSE, warning = FALSE}
temp_int <- l2_residuals_i %>% 
  tibble::rownames_to_column(var="outlier") %>% 
  mutate(is_outlier = ifelse(is_outlier(`(Intercept)`), `(Intercept)`, as.numeric(NA)))
temp_int$outlier[which(is.na(temp_int$is_outlier))] <- as.numeric(NA)

ggplot(temp_int, aes(y = `(Intercept)`, x = 0)) +
  geom_boxplot()  +
  geom_text(aes(label = outlier), na.rm = TRUE, nudge_y = 0.2)
```
Item 2 is an outlier (this item is the word "axes").

#### Flag outliers-participants (slope)
```{r}
is_outlier <- function(x) {
  return(x < quantile(x, 0.25) - 1.5 * IQR(x) | x > quantile(x, 0.75) + 1.5 * IQR(x))
}

temp_int <- l2_residuals_p %>% 
  tibble::rownames_to_column(var="outlier") %>% 
  mutate(is_outlier = ifelse(is_outlier(`Context_dev`), `Context_dev`, as.numeric(NA)))
temp_int$outlier[which(is.na(temp_int$is_outlier))] <- as.numeric(NA)

ggplot(temp_int, aes(y = `(Intercept)`, x = 0)) +
  geom_boxplot()  +
  geom_text(aes(label = outlier), na.rm = TRUE, nudge_y = 0.2)
```
No outlier flagged.

##### As an aside, what if we removed these outliers?
Removing both participants and item outliers creates a convergence issue (remember that our sample size is small to begin with). Thus, we first tried removing participants 8 and 29.

```{r}
Model3bis <- update(Model3, . ~ ., data=filter(PSYC746_final, Subject != c(8, 29)))
summary(Model3bis)
```
Removing outlier participants does not drastically change the effects: the effect of 
language Group, which did not reach significance in Model 3 (p = .06) is now at .10. The interaction coefficient is similar to the one in Model 3, and it still does not reach significance.

We also tried removing the outlier item.
```{r}
Model3bis <- update(Model3, . ~ ., data=filter(PSYC746_final, ITEM != 2))
summary(Model3bis)
```
Removing this item does not significantly impact the effects observed in Model 3.

#### Relatedness
In what follows, we checked whether:
-L2 predictors were unrelated to L2 residuals
-L2 predictors were not related to L1 residuals 
-L2 residuals were unrelated to L1 residuals
-L2 residuals were independent from one another and across L2 units

##### Relatedness of L2 predictors and L2 residuals
**Relatedness of L2 predictors (context) and L2 residuals (intercept participant)**
```{r}
l2_residuals_rowname_p <- rownames_to_column(l2_residuals_p) %>% 
  rename("Subject" = rowname) %>%
  mutate("Subject" = as.integer(Subject)) %>% 
  rename(L2resid_p = '(Intercept)') %>%
  rename(L2resid_slope = 'Context_dev') %>%
  as_tibble()

l2_mix <- merge(l2_residuals_rowname_p, PSYC746_final, by = "Subject")

l2_mix %>% ggplot(mapping = aes(x = L2resid_p, y = Context_dev)) +
  geom_point() +
  labs(x = "Residuals participants", y = "Context")
cor.test(l2_mix$L2resid_p, l2_mix$Context_dev)
```
There is no significant correlation between L2 residuals by participant and Context.

**Relatedness of L2 predictors (group) and L2 residuals (intercept participant)**
```{r}
l2_mix %>% ggplot(mapping = aes(x = L2resid_p, y = Group_dev)) +
  geom_point() +
  labs(x = "Residuals participants", y = "Group")
cor.test(l2_mix$L2resid_p, l2_mix$Group_dev) 
```
There is no significant correlation between L2 residuals by participant and Language Group.

**Relatedness of l2 predictor (context) and l2_residuals (intercept items)**
```{r}
l2_residuals_rowname_i <- rownames_to_column(l2_residuals_i) %>% 
  rename("ITEM" = rowname) %>%
  mutate("ITEM" = as.integer(ITEM)) %>% 
  rename(L2resid_i = '(Intercept)') %>%
  as_tibble()

l2_mix <- merge(l2_residuals_rowname_i, PSYC746_final, by = "ITEM")

l2_mix %>% ggplot(mapping = aes(x = L2resid_i, y = Context_dev)) +
  geom_point() +
  labs(x = "Residuals items", y = "Context")
cor.test(l2_mix$L2resid_i, l2_mix$Context_dev) 
```
There is no significant correlation between L2 residuals by item and Context.

**Relatedness of L2 predictors (group) and L2 residuals (intercept items)**
```{r}
l2_mix %>% ggplot(mapping = aes(x = L2resid_i, y = Group_dev)) +
  geom_point() +
  labs(x = "Residuals items", y = "Group")
cor.test(l2_mix$L2resid_i, l2_mix$Group_dev) 
```
There is no significant correlation between L2 residuals by item and Language Group.

**Relatedness of L2 prediction (context) and L2_residuals (slope context participant)**
```{r}
l2_mix <- merge(l2_residuals_rowname_p, PSYC746_final, by = "Subject")
l2_mix %>% ggplot(mapping = aes(x = L2resid_slope, y = Context_dev)) +
  geom_point() +
  labs(x = "Residual slope by participants", y = "Context")
cor.test(l2_mix$L2resid_slope, l2_mix$Context_dev)
```
There is no significant correlation between L2 residuals (random slope for Context by participants) and Context.

**Relatedness of L2 predictor (group) and L2 residuals (slope context participant)**
```{r}
l2_mix %>% ggplot(mapping = aes(x = L2resid_slope, y = Group_dev)) +
  geom_point() +
  labs(x = "Residual slope by participants", y = "Group")
cor.test(l2_mix$L2resid_slope, l2_mix$Group_dev)
```
There is no significant correlation between L2 residuals (random slope for Context by participants) and Language Group.

##### Relatedness of L2 predictors and L1 residuals

**Relatedness of L2 predictors (context) and L1 residuals**
```{r}
PSYC746_final %>% ggplot(mapping = aes(x = l1resid, y = Context_dev)) +
  geom_point() +
  labs(x = "Residuals", y = "Context")
cor.test(PSYC746_final$l1resid, PSYC746_final$Context_dev) 
```
There is no significant correlation between L1 residuals and Context.

**Relatedness of L2 predictors (group) and L1 residuals**
```{r}
PSYC746_final %>% ggplot(mapping = aes(x = l1resid, y = Group_dev)) +
  geom_point() +
  labs(x = "Residuals", y = "Group")
cor.test(PSYC746_final$l1resid, PSYC746_final$Group_dev) 
```
There is no significant correlation between L1 residuals and Language Group.

##### Relatedness of L1 and L2 residuals
**Relatedness of l1_residuals and l2_residuals (intercept participants)**
```{r, echo = FALSE, warning = FALSE}
l2_residuals_rowname <- rownames_to_column(l2_residuals_p) %>% 
  rename("Subject" = rowname) %>%
  mutate("Subject" = as.integer(Subject)) %>% 
  rename(intercept_residual = `(Intercept)`) %>%
  rename(slope_residual = `Context_dev`) %>%
  as_tibble()

PSYC746_final <- full_join(PSYC746_final, l2_residuals_rowname, by = "Subject") 

PSYC746_final %>%
  ggplot() + 
  geom_point(mapping = aes(x = l1resid, y = intercept_residual))
cor.test(PSYC746_final$l1resid, PSYC746_final$intercept_residual) 
```

There is no significant correlation between L1 residuals and L2 residuals (random intercept by participant). Again, the two outliers are clearly identifiable. 

**Relatedness of l1_residuals and l2_residuals (intercept items)**
```{r, echo = FALSE, warning = FALSE}
l2_residuals_rowname2 <- rownames_to_column(l2_residuals_i) %>% 
  rename("ITEM" = rowname) %>%
  mutate("ITEM" = as.integer(ITEM)) %>% 
  rename(intercept_residual2 = `(Intercept)`) %>%
  as_tibble()

PSYC746_final <- full_join(PSYC746_final, l2_residuals_rowname2) 

PSYC746_final %>%
  ggplot() + 
  geom_point(mapping = aes(x = l1resid, y = intercept_residual2))
cor.test(PSYC746_final$l1resid, PSYC746_final$intercept_residual2) 
```
There is a significant correlation between L1 residuals and the random intercept by item, although the correlation is very close to 0 (p = .09).

**Relatedness of l1_residuals and l2_residuals (slope participants)**
```{r, echo = FALSE, warning = FALSE}
PSYC746_final %>%
  ggplot() + 
  geom_point(mapping = aes(x = l1resid, y = slope_residual))
cor.test(PSYC746_final$l1resid, PSYC746_final$slope_residual) 
```
There is no significant correlation between L1 residuals and L2 residuals (random slope for Context by participant). 

##### Relatedness L2 residuals and L2 residuals

**L2 intercept (participant)-L2 slope (participants)**
```{r}
l2_mix %>% ggplot(mapping = aes(x = L2resid_p, y = L2resid_slope)) +
  geom_point() +
  labs(x = "Intercept participants", y = "Slope Context by participant")
cor.test(l2_mix$L2resid_p, l2_mix$L2resid_slope) 
```
There is a moderate negative correlation (significant), suggesting that the higher the mean log TRT value for an individual (i.e., the slower the individual), the smaller the effect of context for this individual. This is a violation of the assumption of independence of errors.

**L2 intercept (participant)-L2 intercept (item)**
```{r}
l2_mix2 <- merge(l2_mix, l2_residuals_rowname_i, by = "ITEM")
l2_mix2 %>% ggplot(mapping = aes(x = L2resid_p, y = L2resid_i)) +
  geom_point() +
  labs(x = "Intercept participants", y = "Intercept items")
cor.test(l2_mix2$L2resid_p, l2_mix2$L2resid_i)
```
There is no correlation between the random intercept by participant and the random intercept by item. 

**L2 intercept (items)-L2 slope (participants)**
```{r}
l2_mix2 %>% ggplot(mapping = aes(x = L2resid_i, y = L2resid_slope)) +
  geom_point() +
  labs(x = "Intercept items", y = "Slope Context by participant")
cor.test(l2_mix2$L2resid_i, l2_mix2$L2resid_slope) 
```
There is no correlation between the random intercept by item and the random slope for Context by item.
